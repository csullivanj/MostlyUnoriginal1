<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <title>Mostly Unoriginal</title>
    <link rel="stylesheet" href="general.css">
    <link rel="shortcut icon" type="image/png" href="diderot1.png"/>
    <script src="general.js" defer></script>
</head>



<body>
    <header>
        <div class="top-bar">
            <div class="left-section">
                <a href="https://www.britannica.com/biography/Denis-Diderot" target="_blank" rel="noopener noreferrer">
                  <img class="diderot" src="diderot1.png">
                  </a>
                  <a href="https://www.mostlyunoriginal.com" class="title">Mostly <br> Unoriginal <br>
                  </a>
                </div>
            <nav>
                <ul>
                    <li><a href="#" >Why</a></li>
                    <li><a href="#">Deep Dives</a></li>
                    <li><a href="#">Essays</a></li>
                    <li><a href="#">Books</a></li>
                    <li><a href="#">Quotes</a></li>
                    <li><a href="#">Contact</a></li>
                </ul>
            </nav>
        </div>
    </header>
  
    <div id="progress-bar"></div>

    <div class="main-content">
      <div class="main-column">
        <p class="title1">
          On the Origin of Species
        </p>
        <p class="date1">
          May 9, 2024
        </p>
        <p>
        Particularly given its recent surprise successes, there’s a somewhat widespread belief that eventually AI will be able to “do everything”, or at least everything we currently do. So what about science? Over the centuries we humans have made incremental progress, gradually building up what’s now essentially the single largest intellectual edifice of our civilization. But despite all our efforts, there are still all sorts of scientific questions that remain. So can AI now come in and just solve all of them?

        <br><br> To this ultimate question we’re going to see that the answer is inevitably and firmly no. But that certainly doesn’t mean AI can’t importantly help the progress of science. At a very practical level, for example, LLMs provide a new kind of linguistic interface to the computational capabilities that we’ve spent so long building in the Wolfram Language. And through their knowledge of “conventional scientific wisdom” LLMs can often provide what amounts to very high-level “autocomplete” for filling in “conventional answers” or “conventional next steps” in scientific work.
        
        <br><br> But what I want to do here is to discuss what amount to deeper questions about AI in science. Three centuries ago science was transformed by the idea of representing the world using mathematics. And in our times we’re in the middle of a major transformation to a fundamentally computational representation of the world (and, yes, that’s what our Wolfram Language computational language is all about). So how does AI stack up? Should we think of it essentially as a practical tool for accessing existing methods, or does it provide something fundamentally new for science?
        
        <br><br> My goal here is to explore and assess what AI can and can’t be expected to do in science. I’m going to consider a number of specific examples, simplified to bring out the essence of what is (or isn’t) going on. I’m going to talk about intuition and expectations based on what we’ve seen so far. And I’m going to discuss some of the theoretical—and in some ways philosophical—underpinnings of what’s possible and what’s not.
        
        So what do I actually even mean by “AI” here? In the past, anything seriously computational was often considered “AI”, in which case, for example, what we’ve done for so long with our Wolfram Language computational language would qualify—as would all my “ruliological” study of simple programs in the computational universe. But here for the most part I’m going to adopt a narrower definition—and say that AI is something based on machine learning (and usually implemented with neural networks), that’s been incrementally trained from examples it’s been given. Often I’ll add another piece as well: that those examples include either a large corpus of human-generated scientific text, etc., or a corpus of actual experience about things that happen in the world—or, in other words, that in addition to being a “raw learning machine” the AI is something that’s already learned from lots of human-aligned knowledge.
        Particularly given its recent surprise successes, there’s a somewhat widespread belief that eventually AI will be able to “do everything”, or at least everything we currently do. So what about science? Over the centuries we humans have made incremental progress, gradually building up what’s now essentially the single largest intellectual edifice of our civilization. But despite all our efforts, there are still all sorts of scientific questions that remain. So can AI now come in and just solve all of them?

        <br><br> To this ultimate question we’re going to see that the answer is inevitably and firmly no. But that certainly doesn’t mean AI can’t importantly help the progress of science. At a very practical level, for example, LLMs provide a new kind of linguistic interface to the computational capabilities that we’ve spent so long building in the Wolfram Language. And through their knowledge of “conventional scientific wisdom” LLMs can often provide what amounts to very high-level “autocomplete” for filling in “conventional answers” or “conventional next steps” in scientific work.
        
        <br><br> But what I want to do here is to discuss what amount to deeper questions about AI in science. Three centuries ago science was transformed by the idea of representing the world using mathematics. And in our times we’re in the middle of a major transformation to a fundamentally computational representation of the world (and, yes, that’s what our Wolfram Language computational language is all about). So how does AI stack up? Should we think of it essentially as a practical tool for accessing existing methods, or does it provide something fundamentally new for science?
        
        <br><br> My goal here is to explore and assess what AI can and can’t be expected to do in science. I’m going to consider a number of specific examples, simplified to bring out the essence of what is (or isn’t) going on. I’m going to talk about intuition and expectations based on what we’ve seen so far. And I’m going to discuss some of the theoretical—and in some ways philosophical—underpinnings of what’s possible and what’s not.
        
        So what do I actually even mean by “AI” here? In the past, anything seriously computational was often considered “AI”, in which case, for example, what we’ve done for so long with our Wolfram Language computational language would qualify—as would all my “ruliological” study of simple programs in the computational universe. But here for the most part I’m going to adopt a narrower definition—and say that AI is something based on machine learning (and usually implemented with neural networks), that’s been incrementally trained from examples it’s been given. Often I’ll add another piece as well: that those examples include either a large corpus of human-generated scientific text, etc., or a corpus of actual experience about things that happen in the world—or, in other words, that in addition to being a “raw learning machine” the AI is something that’s already learned from lots of human-aligned knowledge.
        Particularly given its recent surprise successes, there’s a somewhat widespread belief that eventually AI will be able to “do everything”, or at least everything we currently do. So what about science? Over the centuries we humans have made incremental progress, gradually building up what’s now essentially the single largest intellectual edifice of our civilization. But despite all our efforts, there are still all sorts of scientific questions that remain. So can AI now come in and just solve all of them?

        <br><br> To this ultimate question we’re going to see that the answer is inevitably and firmly no. But that certainly doesn’t mean AI can’t importantly help the progress of science. At a very practical level, for example, LLMs provide a new kind of linguistic interface to the computational capabilities that we’ve spent so long building in the Wolfram Language. And through their knowledge of “conventional scientific wisdom” LLMs can often provide what amounts to very high-level “autocomplete” for filling in “conventional answers” or “conventional next steps” in scientific work.
        
        <br><br> But what I want to do here is to discuss what amount to deeper questions about AI in science. Three centuries ago science was transformed by the idea of representing the world using mathematics. And in our times we’re in the middle of a major transformation to a fundamentally computational representation of the world (and, yes, that’s what our Wolfram Language computational language is all about). So how does AI stack up? Should we think of it essentially as a practical tool for accessing existing methods, or does it provide something fundamentally new for science?
        
        <br><br> My goal here is to explore and assess what AI can and can’t be expected to do in science. I’m going to consider a number of specific examples, simplified to bring out the essence of what is (or isn’t) going on. I’m going to talk about intuition and expectations based on what we’ve seen so far. And I’m going to discuss some of the theoretical—and in some ways philosophical—underpinnings of what’s possible and what’s not.
        
        So what do I actually even mean by “AI” here? In the past, anything seriously computational was often considered “AI”, in which case, for example, what we’ve done for so long with our Wolfram Language computational language would qualify—as would all my “ruliological” study of simple programs in the computational universe. But here for the most part I’m going to adopt a narrower definition—and say that AI is something based on machine learning (and usually implemented with neural networks), that’s been incrementally trained from examples it’s been given. Often I’ll add another piece as well: that those examples include either a large corpus of human-generated scientific text, etc., or a corpus of actual experience about things that happen in the world—or, in other words, that in addition to being a “raw learning machine” the AI is something that’s already learned from lots of human-aligned knowledge.
      </p>
      </div>






      <div class="side-bar">
        <br><br><br><br><br>
        <p class="recent">
          Popular deep dives
        </p>

        <div class="recent-work">
          <div class="article">
            <a class="articlelinks"><img src="grid.jpg" alt="Article 1"></a>
            <div class="article-info">
            <a href="#">Australia's Electricity Grid</a>
            <span class="date">March 1, 2024</span>
          </div>
          </div>


          <div class="article">
            <a class="articlelinks"><img src="birds.jpg" alt="Article 2"></a>
            <div class="article-info">
            <a href="#">Migration Patterns</a>
            <span class="date">March 1, 2024</span>
          </div>
          </div>


          <div class="recent-work">
            <div class="article">
              <a class="articlelinks"><img src="grid.jpg" alt="Article 1"></a>
              <div class="article-info">
              <a href="#">Australia's Electricity Grid</a>
              <span class="date">March 1, 2024</span>
            </div>
            </div>

            <div class="article">
              <a class="articlelinks"><img src="birds.jpg" alt="Article 2"></a>
              <div class="article-info">
              <a href="#">Migration Patterns</a>
              <span class="date">March 1, 2024</span>
            </div>
            </div>

            <div class="recent-work">
              <div class="article">
                <a class="articlelinks"><img src="grid.jpg" alt="Article 1"></a>
                <div class="article-info">
                <a href="#">Australia's Electricity Grid</a>
                <span class="date">March 1, 2024</span>
              </div>
              </div>

              <div class="article">
                <a class="articlelinks"><img src="birds.jpg" alt="Article 2"></a>
                <div class="article-info">
                <a href="#">Migration Patterns</a>
                <span class="date">March 1, 2024</span>
              </div>
              </div>

              <div class="recent-work">
                <div class="article">
                  <a class="articlelinks"><img src="grid.jpg" alt="Article 1"></a>
                  <div class="article-info">
                  <a href="#">Australia's Electricity Grid</a>
                  <span class="date">March 1, 2024</span>
                </div>
                </div>
      </div>
    </div>
</body>
</html>
